# 3

![3.1](3.1.png)

![3.2](3.2.png)

![3.3](3.3.png)

![3.4](3.4.png)

![3.5](3.5.png)

![3.6](3.6.png)

![3.7](3.7.png)

При вимкненні однієї ноди чи кількох послідовно втрат даних немає - лишаються бекапи, які ширяться між нодами

![3.8](3.8.png)

![3.9](3.9.png)

При одночасному вимкненні двох втрачається частина даних - ті, які були збереженні на одній із тих, що вимкнулися, а бекап яких був на іншій з тих, що вимкнулися

# 4

![4.1](4.1.png)

![4.2](4.2.png)

![4.3](4.3.png)

Бачимо data race

# 5

![5.1](5.1.png)

![5.2](5.2.png)

![5.3](5.3.png)

Нема data race, але працює більше двох хвилин

# 6

![6.1](6.1.png)

![6.2](6.2.png)

![6.3](6.3.png)

Теж нема data race, аде працює 20 секунд

Зросла кількість запитів - працює схоже до неблокуючих засобів синхронізації, із compare and swap

# 7

Без блокувань є data race, з песимістичним і оптимістичним нема, але оптимістичний набагато швидший

# 8

![8.1](8.1.png)

Кожен клієнт, як там встигає, так і забирає з черги елемент - тож виходить, що вони зазвичай чергуються по одному елементу, але деколи один встигає взяти два, поки інший жодного

![8.2](8.2.png)

Чомусь якщо немає клієнтів, що забирають із черги, hazelcast не робить, аби запис у чергу блокувався при досягненні границі розміру черги. Тож таке враження, що після 10 записів решта даних починає зникати замість того, аби заблокувати запис (бачимо багато rejected на панелі керування). Напевно, це треба реалізовувати самостійно, якщо робити production ready.
